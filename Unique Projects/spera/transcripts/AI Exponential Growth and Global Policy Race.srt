1
00:00:00,000 --> 00:00:05,839
The pace of AI development feels less like a steady jog and more like a, I don't know, a rocket launch.

2
00:00:05,900 --> 00:00:06,639
It really does.

3
00:00:06,679 --> 00:00:11,339
If you, like us, feel like you're constantly scrambling to keep up, you are not wrong.

4
00:00:12,480 --> 00:00:13,960
Welcome to the Deep Dive.

5
00:00:14,140 --> 00:00:24,179
Today, we've taken the comprehensive 2024 and 2025 AI index reports, plus a thick stack of global regulatory documents, and we've distilled them.

6
00:00:24,300 --> 00:00:25,960
And our mission today is pretty simple, right?

7
00:00:25,960 --> 00:00:32,960
Yeah, to give you a clear, high-level map of this volatile landscape, we're tracking three major forces.

8
00:00:33,259 --> 00:00:36,140
The staggering velocity of AI capability growth.

9
00:00:36,259 --> 00:00:39,460
The serious documented challenges those advances are creating.

10
00:00:39,560 --> 00:00:45,039
And finally, the concrete global policy and technical solutions that are, frankly, racing to catch up.

11
00:00:45,119 --> 00:00:46,240
OK, let's unpack this.

12
00:00:46,259 --> 00:00:52,159
We have to start with velocity because the sheer speed is the foundation of every other problem and solution we're going to talk about.

13
00:00:52,240 --> 00:00:53,000
It really is.

14
00:00:53,000 --> 00:00:59,039
The biggest headline we pulled from those documents was about the shift in who is actually driving this rapid development.

15
00:00:59,219 --> 00:01:00,700
It's a clear power consolidation.

16
00:01:01,579 --> 00:01:08,159
If you look at the research output just from 2023, industry produced 51 notable machine learning models.

17
00:01:08,280 --> 00:01:10,340
51. And academia.

18
00:01:10,620 --> 00:01:15,099
Academia, which, you know, historically drove fundamental research, produced only 15.

19
00:01:15,280 --> 00:01:19,299
So the capital and the raw compute required to train these frontier models.

20
00:01:19,340 --> 00:01:19,700
Yeah.

21
00:01:19,900 --> 00:01:21,060
It's all centralized now.

22
00:01:21,579 --> 00:01:23,540
Almost entirely in the private sector, yes.

23
00:01:23,640 --> 00:01:28,180
And when you look at the compute required for this, the numbers just become truly dizzying.

24
00:01:28,340 --> 00:01:28,799
They do.

25
00:01:29,219 --> 00:01:37,159
The sources clearly show that the training compute needed for the most notable AI models is doubling approximately every five months.

26
00:01:37,480 --> 00:01:38,560
Doubling every five months.

27
00:01:38,780 --> 00:01:39,879
Just think about that pace.

28
00:01:40,040 --> 00:01:44,420
I mean, if your rent or your salary doubled that fast, the world would fundamentally change in a year.

29
00:01:44,799 --> 00:01:49,180
This is an exponential trend that utterly dwarfs older metrics like Moore's law.

30
00:01:49,180 --> 00:01:52,099
Which means the capability floor is constantly rising.

31
00:01:52,519 --> 00:01:52,519


32
00:01:52,540 --> 00:01:57,459
And critically, AI is achieving benchmarks that were previously strictly human domains.

33
00:01:57,640 --> 00:01:59,379
We're not talking about Go or chess anymore.

34
00:01:59,680 --> 00:02:00,299
No, not at all.

35
00:02:00,319 --> 00:02:02,659
We're talking about high stakes abstract reasoning.

36
00:02:03,379 --> 00:02:08,599
What's fascinating here is exactly where AI is conquering these traditional human benchmarks.

37
00:02:08,860 --> 00:02:11,900
So these are the things we thought were SACE, the things that made us uniquely human.

38
00:02:11,979 --> 00:02:13,099
For years, yeah.

39
00:02:13,099 --> 00:02:19,199
Tasks involving complex mathematical and abstract reasoning were considered strongholds for human expertise.

40
00:02:19,680 --> 00:02:21,919
But that barrier is breaking down fast.

41
00:02:22,120 --> 00:02:23,500
So give us the concrete scorecard.

42
00:02:23,639 --> 00:02:25,319
Where did the machine finally win?

43
00:02:25,680 --> 00:02:28,659
The documents pinpoint a few key conquests.

44
00:02:29,080 --> 00:02:32,060
In high level mathematics, the data is just astonishing.

45
00:02:32,659 --> 00:02:41,080
OpenAI's 03 mini model, which came out in January 2025, achieved 97.9% accuracy on the challenging math data set.

46
00:02:41,080 --> 00:02:43,560
97.9% and the human baseline is what?

47
00:02:43,719 --> 00:02:45,300
The human baseline is 90%.

48
00:02:45,300 --> 00:02:46,740
So it's officially surpassed us.

49
00:02:46,740 --> 00:02:48,560
That's competition level math proficiency.

50
00:02:48,699 --> 00:02:54,780
So we have an AI that can essentially pass advanced math exams better than the average top student.

51
00:02:55,099 --> 00:02:58,560
What about reasoning and knowledge application?

52
00:02:58,879 --> 00:03:02,939
Well, beyond CureMath, we saw major advances in visual and conceptual reasoning.

53
00:03:03,379 --> 00:03:09,840
In 2024, AI systems finally match the human baseline on visual common sense reasoning or VCR.

54
00:03:09,840 --> 00:03:11,319
And VCR is.

55
00:03:11,599 --> 00:03:14,520
That's about understanding subtle relationships and images, right?

56
00:03:14,539 --> 00:03:15,719
Not just identifying objects.

57
00:03:16,199 --> 00:03:16,319
Exactly.

58
00:03:16,719 --> 00:03:25,620
And for abstract pattern recognition, OpenAI's 03 model got a 75.7% on the previously difficult ARC-AGI benchmark.

59
00:03:25,860 --> 00:03:29,500
And this proficiency is translating directly into high stakes fields like medicine.

60
00:03:29,659 --> 00:03:30,139
Absolutely.

61
00:03:30,340 --> 00:03:33,460
Which is where mistakes have immediate life altering consequences.

62
00:03:33,879 --> 00:03:37,039
Clinical large language models are improving at a breathtaking pace.

63
00:03:37,139 --> 00:03:38,419
OK, so what are the numbers there?

64
00:03:38,419 --> 00:03:40,060
Take the MedQA benchmark.

65
00:03:40,659 --> 00:03:43,400
It's essentially the equivalent of a clinical licensing exam.

66
00:03:44,039 --> 00:03:50,580
In 2024, OpenAI's 01 model achieved a state of the art 96.0% accuracy on it.

67
00:03:51,020 --> 00:03:51,539
96%.

68
00:03:51,539 --> 00:03:51,979
Yes.

69
00:03:52,300 --> 00:03:59,659
And what makes that data point so compelling is that represents a stunning 28.4% point improvement since just late 2022.

70
00:03:59,740 --> 00:04:00,560
And just over a year.

71
00:04:00,699 --> 00:04:01,000
Yeah.

72
00:04:01,400 --> 00:04:05,460
This signals that MedQA, a key benchmark for clinical knowledge, may soon be saturated.

73
00:04:05,500 --> 00:04:08,080
We just need a harder test to even measure progress now.

74
00:04:08,080 --> 00:04:12,280
Which means we're going to need even harder challenges to track real capability.

75
00:04:12,539 --> 00:04:13,439
But here's the thing.

76
00:04:14,020 --> 00:04:16,199
The models aren't just getting better, are they?

77
00:04:16,360 --> 00:04:18,740
They're getting smarter and much, much more efficient.

78
00:04:19,000 --> 00:04:20,000
That's the perfect transition.

79
00:04:20,220 --> 00:04:21,939
The trend used to be scaling up.

80
00:04:22,060 --> 00:04:23,740
Just throw more parameters at the problem.

81
00:04:23,839 --> 00:04:26,579
Now the goal is scaling down without losing performance.

82
00:04:26,819 --> 00:04:28,220
So tell us about that size reduction.

83
00:04:28,420 --> 00:04:29,759
That was a huge headline.

84
00:04:29,860 --> 00:04:30,459
It was.

85
00:04:30,600 --> 00:04:36,860
The reports highlight the smallest model capable of surpassing 60% on the comprehensive MMLU benchmark.

86
00:04:36,860 --> 00:04:44,579
In 2022, that model was Google's Paul M, which had a massive 540 billion parameters.

87
00:04:44,899 --> 00:04:45,699
540 billion.

88
00:04:45,839 --> 00:04:46,040
OK.

89
00:04:46,220 --> 00:04:51,459
Just two years later in 2024, that same threshold was met by Microsoft's Phi 3 Mini.

90
00:04:51,899 --> 00:04:54,100
It has just 3.8 billion parameters.

91
00:04:54,180 --> 00:04:56,800
That's a 142 fold reduction in model size.

92
00:04:56,860 --> 00:04:57,620
In two years.

93
00:04:57,860 --> 00:04:59,019
That isn't just a fun statistic.

94
00:04:59,180 --> 00:05:01,620
No, it's a huge shift in capability accessibility.

95
00:05:01,740 --> 00:05:02,000
Right.

96
00:05:02,000 --> 00:05:07,620
It means powerful, highly capable AI is now accessible outside of just five hyperscale labs.

97
00:05:08,120 --> 00:05:12,660
It changes the security calculus, the distribution of power, and frankly, who can afford to innovate?

98
00:05:13,040 --> 00:05:18,019
And that accessibility pushes us directly toward the urgent challenges these capabilities are creating.

99
00:05:18,300 --> 00:05:18,860
Precisely.

100
00:05:19,000 --> 00:05:24,040
But this lightning fast progress, as you said, it always comes with a wake of serious trouble.

101
00:05:24,459 --> 00:05:31,220
Let's pivot now to the urgent challenges, because the acceleration of capability has been mirrored by an explosion in risk.

102
00:05:31,220 --> 00:05:32,199
It really has.

103
00:05:32,699 --> 00:05:41,439
According to the AI Incidence Database, the number of reported AI-related incidents hit a record high of 233 in 2024 alone.

104
00:05:41,540 --> 00:05:43,220
And that's a... what was the percentage increase?

105
00:05:43,500 --> 00:05:46,220
A 56.4% increase over 2023.

106
00:05:46,480 --> 00:05:49,379
And we have to remember that's based only on reported incidents.

107
00:05:49,639 --> 00:05:51,180
The real number is likely much higher.

108
00:05:51,399 --> 00:05:57,620
A 56% increase in failures in a single year, even as models are getting demonstrably better at math and medicine.

109
00:05:57,660 --> 00:05:58,379
What's going on there?

110
00:05:58,600 --> 00:06:00,399
That dichotomy is the core of the problem.

111
00:06:00,399 --> 00:06:03,959
We lack standardized tools to even measure safety consistently.

112
00:06:04,000 --> 00:06:05,399
We know the risks are multiplying.

113
00:06:05,779 --> 00:06:09,800
But we lack standardized responsible AI or RAI benchmarks.

114
00:06:10,100 --> 00:06:14,740
So the developers OpenAI, Google, Anthropic, they're all testing against different internal standards.

115
00:06:15,180 --> 00:06:16,180
Largely, yes.

116
00:06:16,500 --> 00:06:18,220
They're using proprietary benchmarks.

117
00:06:18,980 --> 00:06:26,819
So if a model fails a safety test in one lab, we can't systematically compare that risk to a similar model from a competing lab.

118
00:06:27,000 --> 00:06:29,779
Which makes systematic risk comparison almost impossible.

119
00:06:29,779 --> 00:06:30,600
Exactly.

120
00:06:30,879 --> 00:06:34,459
And that's critical because these models are still highly vulnerable.

121
00:06:34,939 --> 00:06:38,980
They struggle acutely with factuality, what we all call hallucination.

122
00:06:39,399 --> 00:06:43,259
And they are highly susceptible to sophisticated adversarial attacks.

123
00:06:43,439 --> 00:06:44,339
You mean red teaming.

124
00:06:44,639 --> 00:06:45,040
Yes.

125
00:06:45,360 --> 00:06:49,199
Clever red teaming prompts that can get them to bypass their own safety protocols.

126
00:06:49,379 --> 00:06:51,459
What's the real world danger of these bypasses?

127
00:06:51,459 --> 00:06:52,000
What can happen?

128
00:06:52,220 --> 00:06:55,560
Well, the models can be manipulated into revealing sensitive data.

129
00:06:55,560 --> 00:07:04,740
For example, leaking what we call PII, personally identifiable information like phone numbers or private details they might have inadvertently stored from their training data.

130
00:07:05,240 --> 00:07:07,920
Or just generating harmful content.

131
00:07:08,300 --> 00:07:12,660
Beyond safety risks is a fundamental looming challenge to the model's fuel source data.

132
00:07:13,120 --> 00:07:15,639
We're seeing what the sources call a shrinking data commons.

133
00:07:15,939 --> 00:07:16,420
That's right.

134
00:07:16,680 --> 00:07:20,199
For years, models benefited from massive unrestricted web scraping.

135
00:07:20,740 --> 00:07:23,459
But the internet is getting wise to this.

136
00:07:23,459 --> 00:07:28,980
The reports show many major domains implemented protocols to curb data scraping.

137
00:07:29,120 --> 00:07:30,040
So what's the effect of that?

138
00:07:30,480 --> 00:07:34,720
The proportion of restricted tokens in the massive C4 data set jumped drastically.

139
00:07:35,060 --> 00:07:39,240
It went from around 5-7% to somewhere between 20-33%.

140
00:07:39,240 --> 00:07:40,740
So the well is starting to run dry.

141
00:07:40,860 --> 00:07:42,560
This isn't just an abstract problem.

142
00:07:42,879 --> 00:07:46,959
It's a signal that the fundamental resource models need is becoming scarce.

143
00:07:47,160 --> 00:07:50,240
Which is why that next hurdle model collapse is so alarming.

144
00:07:50,540 --> 00:07:51,319
Explain that for us.

145
00:07:51,319 --> 00:07:57,579
Model collapse is what happens when models have to train purely on synthetic or AI generated data

146
00:07:57,579 --> 00:07:58,800
for repeated cycles.

147
00:07:58,920 --> 00:08:00,519
It creates an echo chamber.

148
00:08:00,699 --> 00:08:02,339
And the result is a loss of diversity.

149
00:08:02,600 --> 00:08:06,100
A severe loss of diversity and ultimately degraded output quality.

150
00:08:06,379 --> 00:08:08,860
The models literally forget what reality looks like.

151
00:08:09,220 --> 00:08:13,740
Now the good news is that newer research suggests that if you carefully layer synthetic data on top

152
00:08:13,740 --> 00:08:17,100
of real human generated data, you can mitigate the degradation.

153
00:08:17,480 --> 00:08:20,920
But the pressure to find new high quality human data is immense.

154
00:08:21,319 --> 00:08:26,120
The technical challenges are huge, but the immediate societal and political risks.

155
00:08:26,420 --> 00:08:28,060
They remain front and center.

156
00:08:28,339 --> 00:08:30,920
Especially with major global elections underway.

157
00:08:31,199 --> 00:08:31,860
Oh absolutely.

158
00:08:32,379 --> 00:08:34,580
Deepfakes are now a tool of political conflict.

159
00:08:35,000 --> 00:08:39,799
We've seen concrete examples like the use of contentious AI generated audio clips during

160
00:08:39,799 --> 00:08:41,720
Slovakia's 2023 election.

161
00:08:41,779 --> 00:08:45,039
Where the authenticity was immediately questioned and weaponized.

162
00:08:45,240 --> 00:08:45,659
Exactly.

163
00:08:45,960 --> 00:08:49,299
They're easy to generate and notoriously hard to detect reliably.

164
00:08:49,299 --> 00:08:54,220
And what's worse, the documents note that many popular detectors perform significantly

165
00:08:54,220 --> 00:08:57,659
worse when analyzing deepfakes of certain racial subgroups.

166
00:08:57,779 --> 00:08:59,779
And the risk isn't just external manipulation.

167
00:09:00,299 --> 00:09:03,460
There's the inherent bias found in the commercial models themselves.

168
00:09:03,840 --> 00:09:07,740
Researchers for instance found a significant political bias in chat GPT.

169
00:09:07,820 --> 00:09:12,000
A measurable leaning toward Democrats in the US and the Labour Party in the UK.

170
00:09:12,320 --> 00:09:16,740
So even seemingly neutral tools carry the biases from their training data.

171
00:09:16,840 --> 00:09:17,460
They do.

172
00:09:17,460 --> 00:09:23,120
And this all rolls up into this crucial ethical problem documented in the reports called the

173
00:09:23,120 --> 00:09:23,960
liars dividend.

174
00:09:24,259 --> 00:09:26,500
It's a concept that should genuinely worry everyone.

175
00:09:26,919 --> 00:09:27,620
It really should.

176
00:09:27,940 --> 00:09:31,240
Because deepfake technology exists and is widely known,

177
00:09:31,960 --> 00:09:35,659
individuals, especially public figures, can now deny genuine,

178
00:09:35,860 --> 00:09:39,259
verifiable evidence by falsely claiming it was AI generated.

179
00:09:39,440 --> 00:09:42,059
It erodes public trust in objective reality.

180
00:09:42,139 --> 00:09:45,259
It makes accountability nearly impossible to enforce.

181
00:09:45,259 --> 00:09:48,480
It's a truly corrosive problem for democracy.

182
00:09:48,620 --> 00:09:49,940
Okay, let's shift gears now.

183
00:09:50,240 --> 00:09:52,659
Let's talk about how the world is trying to govern this.

184
00:09:53,120 --> 00:09:53,779
Section 3.

185
00:09:53,840 --> 00:09:59,320
How are we trying to manage this and what positive applications are emerging despite the risks?

186
00:09:59,500 --> 00:10:00,759
Here's where it gets really interesting.

187
00:10:00,899 --> 00:10:01,259
It is.

188
00:10:01,279 --> 00:10:04,399
We are seeing a rapid acceleration of international coordination.

189
00:10:04,779 --> 00:10:08,019
People are recognizing that technical breakthroughs don't respect borders.

190
00:10:08,440 --> 00:10:12,019
So after that first AI safety summit in 2023, things started moving.

191
00:10:12,100 --> 00:10:12,779
Very quickly.

192
00:10:12,779 --> 00:10:18,399
In 2024, AI safety institutes were launched or pledged across the globe in the U.S.,

193
00:10:18,399 --> 00:10:22,080
the U.K., Japan, France, Germany, Italy, Singapore, South Korea,

194
00:10:22,399 --> 00:10:24,220
Australia, Canada, and the EU.

195
00:10:24,519 --> 00:10:27,559
That is massive global coordination happening at speed.

196
00:10:27,740 --> 00:10:30,259
But how are these legislative efforts shaping up?

197
00:10:30,320 --> 00:10:32,360
Are they all converging on a single philosophy?

198
00:10:32,720 --> 00:10:33,360
Not exactly.

199
00:10:33,480 --> 00:10:37,659
Generally, the sources suggest a growing trend toward restrictive legislation

200
00:10:37,659 --> 00:10:40,120
focused on mitigating large-scale harm.

201
00:10:40,120 --> 00:10:43,320
The EU AI Act is the most famous example.

202
00:10:43,600 --> 00:10:43,700
Right.

203
00:10:43,740 --> 00:10:47,320
And it explicitly prohibits what it calls unacceptable risk systems.

204
00:10:47,539 --> 00:10:47,840
Yes.

205
00:10:47,960 --> 00:10:49,639
Things like behavioral manipulators.

206
00:10:50,279 --> 00:10:55,059
And it mandates stringent transparency for high-risk applications and generative AI.

207
00:10:55,279 --> 00:10:57,580
So how does that compare to the U.S. approach?

208
00:10:58,019 --> 00:11:00,000
I know harmonization is a big challenge right now.

209
00:11:00,379 --> 00:11:02,620
That's the core difference the G7 noted.

210
00:11:03,080 --> 00:11:06,340
The U.S. has focused on specific sector-targeted acts.

211
00:11:06,340 --> 00:11:10,200
For instance, the Artificial Intelligence and Biosecurity Risk Assessment Act,

212
00:11:10,539 --> 00:11:13,659
which looks at the threat of AI developing harmful agents.

213
00:11:14,340 --> 00:11:15,539
So things like bioweapons.

214
00:11:15,960 --> 00:11:16,320
Exactly.

215
00:11:16,759 --> 00:11:18,600
Or another one is the Jobs of the Future Act,

216
00:11:18,759 --> 00:11:22,240
which mandates studying AI's direct impact on occupations.

217
00:11:22,320 --> 00:11:23,059
So let me get this straight.

218
00:11:23,220 --> 00:11:27,159
The EU is prescriptive and broad-setting ground rules for all AI systems.

219
00:11:27,259 --> 00:11:27,600
Right.

220
00:11:27,799 --> 00:11:30,379
While the U.S. is more targeted and sector-specific,

221
00:11:30,759 --> 00:11:33,059
focusing on immediate threats and workforce impacts.

222
00:11:33,460 --> 00:11:33,980
Exactly.

223
00:11:33,980 --> 00:11:39,180
And the G7 report explicitly stated that these two differing philosophies,

224
00:11:40,220 --> 00:11:44,019
they're likely to challenge global harmonization efforts for years to come.

225
00:11:44,299 --> 00:11:45,600
It's going to be messy.

226
00:11:45,740 --> 00:11:47,799
It's a messy global rollout, for sure.

227
00:11:48,139 --> 00:11:51,620
But it's critical that we look at the immense positive applications,

228
00:11:51,759 --> 00:11:53,759
especially in high-stakes science and medicine.

229
00:11:54,299 --> 00:11:56,299
The whole story isn't about stopping the bad.

230
00:11:56,559 --> 00:11:58,120
It's also about boosting the good.

231
00:11:58,820 --> 00:11:59,220
Absolutely.

232
00:11:59,580 --> 00:12:02,779
AI is dramatically accelerating scientific discovery.

233
00:12:02,779 --> 00:12:05,379
This is where that compute power really shines.

234
00:12:06,019 --> 00:12:08,120
Google DeepMind's Genome Project, for example,

235
00:12:08,299 --> 00:12:12,759
used AI to discover 2.2 million new crystal structures.

236
00:12:12,820 --> 00:12:13,820
2.2 million.

237
00:12:13,840 --> 00:12:16,000
I mean, that's a game-changer for material science,

238
00:12:16,299 --> 00:12:18,779
potentially unlocking new superconductors or batteries

239
00:12:18,779 --> 00:12:20,940
that human researchers simply overlooked.

240
00:12:21,279 --> 00:12:23,700
And in life-critical fields like weather forecasting,

241
00:12:24,200 --> 00:12:26,500
time is literally measured in lives saved.

242
00:12:26,539 --> 00:12:27,659
That's a perfect example.

243
00:12:27,820 --> 00:12:31,200
Models like GraphCast and GenCast now provide highly accurate

244
00:12:31,200 --> 00:12:34,559
15-day forecasts in minutes rather than the hours needed

245
00:12:34,559 --> 00:12:36,620
by traditional supercomputer simulations.

246
00:12:36,779 --> 00:12:38,639
Which is vital for disaster response.

247
00:12:38,879 --> 00:12:41,120
And climate resilience planning, where speed is everything.

248
00:12:41,440 --> 00:12:46,179
And in healthcare, AI is moving from being a novelty to a necessity.

249
00:12:46,940 --> 00:12:50,179
We've seen an exponential increase in AI-enabled medical devices

250
00:12:50,179 --> 00:12:51,379
approved by the FDA.

251
00:12:51,559 --> 00:12:55,080
And critically, AI is also being deployed to integrate complex factors

252
00:12:55,080 --> 00:12:59,700
that affect treatment outcomes, like social determinants of health or SDOH.

253
00:12:59,700 --> 00:13:02,100
So that means things like a patient's housing situation,

254
00:13:02,919 --> 00:13:04,899
or transport or support systems.

255
00:13:05,320 --> 00:13:05,460
Exactly.

256
00:13:05,659 --> 00:13:08,600
In oncology, for example, AI tools are considering those factors

257
00:13:08,600 --> 00:13:11,399
to create personalized, feasible treatment plans.

258
00:13:11,899 --> 00:13:14,639
This is actively working toward improving health equity,

259
00:13:14,919 --> 00:13:16,539
not just faster diagnoses.

260
00:13:16,539 --> 00:13:19,360
So if we pull back, we are in a tight race.

261
00:13:19,620 --> 00:13:22,639
We have unprecedented capability acceleration on one track.

262
00:13:22,860 --> 00:13:25,580
Leading to massive societal upheaval and risk.

263
00:13:25,700 --> 00:13:29,019
And on the other, a scrambling effort to implement responsible governance

264
00:13:29,019 --> 00:13:30,639
and harness the beneficial outcomes.

265
00:13:31,120 --> 00:13:32,659
The stakes could not be higher.

266
00:13:33,059 --> 00:13:35,139
This raises an important question, you know.

267
00:13:35,340 --> 00:13:38,299
Given that AI is now outperforming humans on high-level coding,

268
00:13:38,500 --> 00:13:40,340
math, and clinical knowledge benchmarks,

269
00:13:40,820 --> 00:13:43,480
yet still struggles with reliable planning and social reasoning.

270
00:13:43,799 --> 00:13:44,820
Things like PlanBench.

271
00:13:45,220 --> 00:13:48,519
What fundamentally human skill beyond technical ability

272
00:13:48,519 --> 00:13:52,980
must society prioritize and cultivate in future education

273
00:13:52,980 --> 00:13:55,799
and workforce development to ensure we guide,

274
00:13:55,799 --> 00:13:58,960
rather than simply deploy, this powerful technology.

275
00:13:59,100 --> 00:14:01,279
It's not about being a better calculator than the machine.

276
00:14:01,440 --> 00:14:02,080
Not anymore.

277
00:14:02,240 --> 00:14:05,179
It's about mastering those uniquely human skills.

278
00:14:05,600 --> 00:14:08,879
Knowing what to calculate next and why it matters to the collective good.

